{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize t-SNE with actual labels\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "statuses = df['status'].iloc[sample_indices]\n",
    "unique_statuses = statuses.unique()\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(unique_statuses)))\n",
    "\n",
    "for i, status in enumerate(unique_statuses):\n",
    "    mask = statuses == status\n",
    "    plt.scatter(features_tsne[mask, 0], features_tsne[mask, 1], \n",
    "               c=[colors[i]], label=status, alpha=0.6, s=30)\n",
    "\n",
    "plt.xlabel('t-SNE Component 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Component 2', fontsize=12)\n",
    "plt.title('t-SNE Visualization of Mental Health Statements (Colored by True Labels)', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863bc7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE for visualization (on a sample for speed)\n",
    "print(\"Applying t-SNE (this may take a few minutes)...\")\n",
    "\n",
    "# Use a sample for faster computation\n",
    "sample_size = min(5000, len(features_pca))\n",
    "sample_indices = np.random.choice(len(features_pca), sample_size, replace=False)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42, verbose=1)\n",
    "features_tsne = tsne.fit_transform(features_pca[sample_indices])\n",
    "\n",
    "print(f\"‚úì t-SNE complete!\")\n",
    "print(f\"  t-SNE shape: {features_tsne.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181aaf48",
   "metadata": {},
   "source": [
    "## 8. Dimensionality Reduction - t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30c94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "print(\"Applying PCA...\")\n",
    "pca = PCA(n_components=0.95)  # Preserve 95% variance\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "print(f\"‚úì PCA complete\")\n",
    "print(f\"  Original dimensions: {features_scaled.shape[1]}\")\n",
    "print(f\"  Reduced dimensions: {features_pca.shape[1]}\")\n",
    "print(f\"  Variance explained: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Plot explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "             pca.explained_variance_ratio_, 'bo-')\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('PCA Scree Plot')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative variance\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "axes[1].plot(range(1, len(cumsum) + 1), cumsum, 'ro-')\n",
    "axes[1].axhline(y=0.95, color='g', linestyle='--', label='95% threshold')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('Cumulative Explained Variance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e50eb",
   "metadata": {},
   "source": [
    "## 7. Dimensionality Reduction - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e315ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(all_features)\n",
    "\n",
    "print(f\"‚úì Features normalized\")\n",
    "print(f\"  Mean: {features_scaled.mean():.6f}\")\n",
    "print(f\"  Std:  {features_scaled.std():.6f}\")\n",
    "print(f\"  Min:  {features_scaled.min():.2f}\")\n",
    "print(f\"  Max:  {features_scaled.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556e42cd",
   "metadata": {},
   "source": [
    "## 6. Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcabaa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features\n",
    "print(\"Combining all features...\")\n",
    "\n",
    "# Linguistic features\n",
    "linguistic_features = features_df.values\n",
    "\n",
    "# TF-IDF features\n",
    "tfidf_features_array = tfidf_array\n",
    "\n",
    "# Combine\n",
    "all_features = np.concatenate([linguistic_features, tfidf_features_array], axis=1)\n",
    "\n",
    "print(f\"‚úì Combined feature matrix shape: {all_features.shape}\")\n",
    "print(f\"  - Linguistic features: {linguistic_features.shape[1]}\")\n",
    "print(f\"  - TF-IDF features: {tfidf_features_array.shape[1]}\")\n",
    "print(f\"  - Total features: {all_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF features\n",
    "print(\"Creating TF-IDF features...\")\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=100,  # Top 100 words\n",
    "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "    min_df=5,  # Minimum document frequency\n",
    "    max_df=0.8  # Maximum document frequency\n",
    ")\n",
    "\n",
    "tfidf_features = tfidf.fit_transform(df['processed_text'])\n",
    "tfidf_array = tfidf_features.toarray()\n",
    "\n",
    "print(f\"‚úì TF-IDF shape: {tfidf_array.shape}\")\n",
    "print(f\"  - Samples: {tfidf_array.shape[0]:,}\")\n",
    "print(f\"  - TF-IDF features: {tfidf_array.shape[1]}\")\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(f\"\\nTop 20 TF-IDF features:\")\n",
    "print(list(feature_names[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69fd109",
   "metadata": {},
   "source": [
    "## 5. TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42efad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract basic text features\n",
    "def extract_features(text):\n",
    "    \"\"\"Extract linguistic features from text\"\"\"\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Basic features\n",
    "    word_count = len(tokens)\n",
    "    char_count = len(text)\n",
    "    avg_word_length = np.mean([len(w) for w in tokens]) if tokens else 0\n",
    "    \n",
    "    # Emotional keywords\n",
    "    negative_words = ['sad', 'depressed', 'hopeless', 'anxious', 'worried', 'scared', \n",
    "                     'hurt', 'pain', 'lonely', 'empty', 'guilty', 'restless', 'nervous']\n",
    "    positive_words = ['happy', 'joy', 'good', 'great', 'love', 'wonderful', 'calm', 'peace']\n",
    "    \n",
    "    negative_count = sum(1 for w in tokens if w in negative_words)\n",
    "    positive_count = sum(1 for w in tokens if w in positive_words)\n",
    "    \n",
    "    # Pronoun usage\n",
    "    first_person = sum(1 for w in tokens if w in ['i', 'me', 'my', 'mine', 'myself'])\n",
    "    \n",
    "    return {\n",
    "        'word_count': word_count,\n",
    "        'char_count': char_count,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'negative_words': negative_count,\n",
    "        'positive_words': positive_count,\n",
    "        'first_person_pronouns': first_person,\n",
    "        'negative_ratio': negative_count / word_count if word_count > 0 else 0,\n",
    "        'first_person_ratio': first_person / word_count if word_count > 0 else 0\n",
    "    }\n",
    "\n",
    "# Extract features for all texts\n",
    "print(\"Extracting linguistic features...\")\n",
    "features_list = df['processed_text'].apply(extract_features).tolist()\n",
    "features_df = pd.DataFrame(features_list)\n",
    "\n",
    "# Combine with original dataframe\n",
    "df_with_features = pd.concat([df, features_df], axis=1)\n",
    "\n",
    "print(\"‚úì Feature extraction complete!\")\n",
    "print(f\"\\nExtracted Features: {list(features_df.columns)}\")\n",
    "print(f\"\\nFeature Statistics:\")\n",
    "print(features_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec3fc9",
   "metadata": {},
   "source": [
    "## 4. Linguistic Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d837cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data if needed\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "print(\"‚úì NLTK data ready\")\n",
    "\n",
    "# Initialize preprocessing tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Complete text preprocessing pipeline\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens \n",
    "              if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing text...\")\n",
    "import re\n",
    "df['processed_text'] = df['statement'].apply(preprocess_text)\n",
    "\n",
    "print(\"‚úì Text preprocessing complete!\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"Original:  {df['statement'].iloc[0]}\")\n",
    "print(f\"Processed: {df['processed_text'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ceaf54",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a4b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar plot\n",
    "df['status'].value_counts().plot(kind='bar', ax=axes[0], color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Mental Health Status Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Mental Health Status', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "df['status'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Mental Health Status Percentage', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nüìä Class Distribution Statistics:\")\n",
    "status_counts = df['status'].value_counts()\n",
    "for status, count in status_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"   {status:25s}: {count:6,} ({percentage:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6804636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../data/Combined Data.csv', index_col=0)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Dataset Shape: {df.shape}\")\n",
    "print(f\"   - Total samples: {len(df):,}\")\n",
    "print(f\"   - Features: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\nüìã Column Names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\nüîç First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nüìà Dataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n‚ùì Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Mental Health Status Distribution:\")\n",
    "print(df['status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3512a038",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b82fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistics\n",
    "from scipy import stats\n",
    "\n",
    "# Settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"‚úì All libraries loaded successfully!\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de59022",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189d2e1f",
   "metadata": {},
   "source": [
    "# üß† Mental Health Biomarker Discovery - Text Analysis\n",
    "\n",
    "## Unsupervised Learning on Mental Health Sentiment Dataset\n",
    "\n",
    "**Dataset:** Kaggle Mental Health Sentiment Analysis  \n",
    "**Size:** 94,025 text statements  \n",
    "**Labels:** Anxiety, Depression, Normal, Bipolar, Personality Disorder, Stress, Suicidal\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Project Goals:\n",
    "1. **Discover hidden subtypes** within each mental health condition\n",
    "2. **Extract linguistic biomarkers** that distinguish conditions\n",
    "3. **Apply unsupervised clustering** to find patterns\n",
    "4. **Validate with statistical analysis**\n",
    "\n",
    "### üìä Pipeline:\n",
    "`Text Data ‚Üí Preprocessing ‚Üí Feature Extraction ‚Üí Dimensionality Reduction ‚Üí Clustering ‚Üí Analysis`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
